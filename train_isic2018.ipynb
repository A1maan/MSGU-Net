{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from model import MSGUNet\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15858017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICSegmentationDataset(Dataset):\n",
    "    VALID_IMG_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\"}  # normal images/masks\n",
    "\n",
    "    def __init__(self, base_dir, split=\"train\", transform=None, mask_transform=None, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_dir (str): Path to the ISIC dataset (ISIC2018 or ISIC2018).\n",
    "            split (str): One of [\"train\", \"test\"] for the final 70:30 split.\n",
    "            transform: Transformations for images.\n",
    "            mask_transform: Transformations for masks.\n",
    "            seed (int): Random seed for shuffling.\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        # Gather images/masks from both train and val folders\n",
    "        all_samples = []\n",
    "        for folder in [\"train\", \"val\"]:\n",
    "            img_dir = os.path.join(base_dir, folder, \"images\")\n",
    "            mask_dir = os.path.join(base_dir, folder, \"masks\")\n",
    "            if not os.path.exists(img_dir) or not os.path.exists(mask_dir):\n",
    "                continue\n",
    "\n",
    "            # Keep only normal images (exclude *_superpixels.png)\n",
    "            images = sorted([\n",
    "                f for f in os.listdir(img_dir)\n",
    "                if os.path.splitext(f)[1].lower() in self.VALID_IMG_EXTENSIONS\n",
    "                and \"_superpixels\" not in f\n",
    "            ])\n",
    "            masks = sorted([\n",
    "                f for f in os.listdir(mask_dir)\n",
    "                if os.path.splitext(f)[1].lower() in self.VALID_IMG_EXTENSIONS\n",
    "                and \"_superpixels\" not in f\n",
    "            ])\n",
    "\n",
    "            for i, m in zip(images, masks):\n",
    "                all_samples.append((os.path.join(img_dir, i), os.path.join(mask_dir, m)))\n",
    "\n",
    "        # Shuffle once to avoid bias before splitting\n",
    "        random.Random(seed).shuffle(all_samples)\n",
    "\n",
    "        # 70:30 split\n",
    "        split_idx = round(0.7 * len(all_samples))\n",
    "        if split == \"train\":\n",
    "            self.samples = all_samples[:split_idx]\n",
    "        elif split == \"test\":\n",
    "            self.samples = all_samples[split_idx:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_2018 = \"/home/aminu_yusuf/msgunet/datasets/ISIC2018\"\n",
    "\n",
    "transform_img = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(p=0.5),   # 50% chance to flip horizontally\n",
    "    T.RandomVerticalFlip(p=0.5),     # 50% chance to flip vertically\n",
    "    T.RandomRotation(degrees=15),    # rotate randomly between -15Â° to +15Â°\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_mask = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])  \n",
    "\n",
    "train_dataset_2018 = ISICSegmentationDataset(\n",
    "    base_dir=base_dir_2018, split=\"train\", transform=transform_img, mask_transform=transform_mask, seed=SEED\n",
    ")\n",
    "test_dataset_2018 = ISICSegmentationDataset(\n",
    "    base_dir=base_dir_2018, split=\"test\", transform=transform_img, mask_transform=transform_mask, seed=SEED\n",
    ")\n",
    "\n",
    "train_loader_2018 = DataLoader(train_dataset_2018, batch_size=8, shuffle=True, drop_last=True, worker_init_fn=seed_worker)\n",
    "test_loader_2018 = DataLoader(test_dataset_2018, batch_size=8, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset_2018))\n",
    "print(\"Test size:\", len(test_dataset_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MSGUNet(in_channels=3, out_channels=1, base_channels=32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()   # binary segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, criterion, optimizer, device, epoch=None, n_epochs=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap loader with tqdm\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{n_epochs} [Train]\", leave=False)\n",
    "    \n",
    "    for imgs, masks in progress_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())  # live update of batch loss\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def eval_epoch(loader, model, criterion, device, epoch=None, n_epochs=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{n_epochs} [Val]\", leave=False)\n",
    "        \n",
    "        for imgs, masks in progress_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return running_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafc09f",
   "metadata": {},
   "source": [
    "# ISIC2018 Training + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d482f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model_path = \"weights/best_model_isic2018.pth\"\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(train_loader_2018, model, criterion, optimizer, device, epoch, n_epochs)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    test_loss = eval_epoch(test_loader_2018, model, criterion, device, epoch, n_epochs)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"âœ… Saved best model at epoch {epoch+1} with Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Optionally save final model too\n",
    "torch.save(model.state_dict(), \"weights/model_isic2018.pth\")\n",
    "print(\"ðŸ’¾ Training complete, final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "imgs, masks = next(iter(test_loader_2018))\n",
    "imgs, masks = imgs.to(device), masks.to(device)\n",
    "with torch.no_grad():\n",
    "    preds = torch.sigmoid(model(imgs))\n",
    "\n",
    "n_samples = min(6, imgs.shape[0])\n",
    "plt.figure(figsize=(12, n_samples * 3))\n",
    "for idx in range(n_samples):\n",
    "    # Base image\n",
    "    plt.subplot(n_samples, 3, idx * 3 + 1)\n",
    "    plt.title(f\"Image {idx+1}\")\n",
    "    plt.imshow(np.transpose(imgs[idx].cpu().numpy(), (1,2,0)))\n",
    "    plt.axis('off')\n",
    "    # Ground truth mask\n",
    "    plt.subplot(n_samples, 3, idx * 3 + 2)\n",
    "    plt.title(\"Mask\")\n",
    "    plt.imshow(masks[idx,0].cpu().numpy(), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    # Prediction\n",
    "    plt.subplot(n_samples, 3, idx * 3 + 3)\n",
    "    plt.title(\"MSGU-Net Prediction\")\n",
    "    plt.imshow(preds[idx,0].cpu().numpy() > 0.5, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/sample_predictions_grid_isic2018.png')\n",
    "plt.show()\n",
    "\n",
    "# After training cell (after training loop and model saving)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/loss_curve_isic2018.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msgunet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
